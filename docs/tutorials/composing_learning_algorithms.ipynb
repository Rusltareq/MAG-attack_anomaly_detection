{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkdnLiKk71g-"
      },
      "source": [
        "##### Copyright 2022 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "0asMuNro71hA"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXslvcRocA-0"
      },
      "source": [
        "# Composing Learning Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XBJJIqwcXKd"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/federated/tutorials/composing_learning_algorithms\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/federated/blob/v0.88.0/docs/tutorials/composing_learning_algorithms.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/federated/blob/v0.88.0/docs/tutorials/composing_learning_algorithms.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/federated/docs/tutorials/composing_learning_algorithms.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnUwFbCAKB2r"
      },
      "source": [
        "## Before you start\n",
        "\n",
        "Before you start, please run the following to make sure that your environment is\n",
        "correctly setup. If you don't see a greeting, please refer to the\n",
        "[Installation](../install.md) guide for instructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZrGitA_KnRO0"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "!pip install --quiet --upgrade tensorflow-federated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HGTM6tWOLo8M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faf2294f-006a-4e75-8a84-0a043fa544ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:jax._src.xla_bridge:Jax plugin configuration error: Plugin module %s could not be loaded\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/xla_bridge.py\", line 428, in discover_pjrt_plugins\n",
            "    plugin_module = importlib.import_module(plugin_module_name)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax_plugins/xla_cuda12/__init__.py\", line 21, in <module>\n",
            "    from jax._src.lib import triton\n",
            "ImportError: cannot import name 'triton' from 'jax._src.lib' (/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py)\n"
          ]
        }
      ],
      "source": [
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr3ztf28fa1F"
      },
      "source": [
        "**NOTE**: This colab has been verified to work with the [latest released version](https://github.com/tensorflow/federated#compatibility) of the `tensorflow_federated` pip package, but the Tensorflow Federated project is still in pre-release development and may not work on `main`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFlTaHe0jV2S"
      },
      "source": [
        "# Composing Learning Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zQlyijofSzI"
      },
      "source": [
        "The [Building Your Own Federated Learning Algorithm Tutorial](https://github.com/tensorflow/federated/blob/v0.88.0/docs/tutorials/building_your_own_federated_learning_algorithm.ipynb) used TFF's federated core to directly implement a version of the Federated Averaging (FedAvg) algorithm.\n",
        "\n",
        "In this tutorial, you will use federated learning components in TFF's API to build federated learning algorithms in a modular manner, without having to re-implement everything from scratch.\n",
        "\n",
        "For the purposes of this tutorial, you will implement a variant of FedAvg that employs gradient clipping through local training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHwcFnLAjqcG"
      },
      "source": [
        "## Learning Algorithm Building Blocks\n",
        "\n",
        "At a high level, many learning algorithms can be separated into 4 separate components, referred to as **building blocks**. These are as follows:\n",
        "\n",
        "1. Distributor (ie. server-to-client communication)\n",
        "1. Client work (ie. local client computation)\n",
        "1. Aggregator (ie. client-to-server communication)\n",
        "1. Finalizer (ie. server computation using aggregated client outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwhOtjlvjboB"
      },
      "source": [
        "While the [Building Your Own Federated Learning Algorithm Tutorial](https://github.com/tensorflow/federated/blob/v0.88.0/docs/tutorials/building_your_own_federated_learning_algorithm.ipynb) implemented all of these building blocks from scratch, this is often unnecessary. Instead, you can re-use building blocks from similar algorithms.\n",
        "\n",
        "In this case, to implement FedAvg with gradient clipping, you only need to modify the **client work** building block. The remaining blocks can be identical to what is used in \"vanilla\" FedAvg."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMnd0RvGlGjK"
      },
      "source": [
        "# Implementing the Client Work\n",
        "\n",
        "First, let's write TF logic that does local model training with gradient clipping. For simplicity, gradients will be clipped have norm at most 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lqZ-c4MphTU"
      },
      "source": [
        "## TF Logic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pIw7QQCqltdV"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def client_update(\n",
        "    model: tff.learning.models.FunctionalModel,\n",
        "    dataset: tf.data.Dataset,\n",
        "    initial_weights: tff.learning.models.ModelWeights,\n",
        "    client_optimizer: tff.learning.optimizers.Optimizer,\n",
        "):\n",
        "  \"\"\"Performs training (using the initial server model weights) on the client's dataset.\"\"\"\n",
        "  # Keep track of the number of examples.\n",
        "  num_examples = 0.0\n",
        "  # Use the client_optimizer to update the local model.\n",
        "  trainable_weights, non_trainable_weights = (\n",
        "      initial_weights.trainable,\n",
        "      initial_weights.non_trainable,\n",
        "  )\n",
        "  optimizer_state = client_optimizer.initialize(\n",
        "      tf.nest.map_structure(lambda x: tf.TensorSpec, trainable_weights)\n",
        "  )\n",
        "  for batch in dataset:\n",
        "    x, y = batch\n",
        "    with tf.GradientTape() as tape:\n",
        "      tape.watch(trainable_weights)\n",
        "      logits = model.predict_on_batch(\n",
        "          model_weights=(trainable_weights, non_trainable_weights),\n",
        "          x=x,\n",
        "          training=True,\n",
        "      )\n",
        "      num_examples += tf.cast(tf.shape(y)[0], tf.float32)\n",
        "      loss = model.loss(output=logits, label=y)\n",
        "\n",
        "    # Compute the corresponding gradient\n",
        "    grads = tape.gradient(loss, trainable_weights)\n",
        "\n",
        "    # Compute the gradient norm and clip\n",
        "    gradient_norm = tf.linalg.global_norm(grads)\n",
        "    if gradient_norm > 1:\n",
        "      grads = tf.nest.map_structure(lambda x: x / gradient_norm, grads)\n",
        "\n",
        "    # Apply the gradient using a client optimizer.\n",
        "    optimizer_state, trainable_weights = client_optimizer.next(\n",
        "        optimizer_state, trainable_weights, grads\n",
        "    )\n",
        "\n",
        "  # Compute the difference between the initial weights and the client weights\n",
        "  client_update = tf.nest.map_structure(\n",
        "      tf.subtract, trainable_weights, initial_weights[0]\n",
        "  )\n",
        "\n",
        "  return tff.learning.templates.ClientResult(\n",
        "      update=client_update, update_weight=num_examples\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe_emK8LpQe0"
      },
      "source": [
        "There are a few important points about the code above. First, it keeps track of the number of examples seen, as this will constitute the *weight* of the client update (when computing an average across clients).\n",
        "\n",
        "Second, it uses [`tff.learning.templates.ClientResult`](https://www.tensorflow.org/federated/api_docs/python/tff/learning/templates/ClientResult) to package the output. This return type is used to standardize client work building blocks in `tff.learning`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5aKjB1Vpiv3"
      },
      "source": [
        "## Creating a ClientWorkProcess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IvXUJAzm8ab"
      },
      "source": [
        "While the TF logic above will do local training with clipping, it still needs to be wrapped in TFF code in order to create the necessary building block.\n",
        "\n",
        "Specifically, the 4 building blocks are represented as a [`tff.templates.MeasuredProcess`](https://www.tensorflow.org/federated/api_docs/python/tff/templates/MeasuredProcess). This means that all 4 blocks have both an `initialize` and `next` function used to instantiate and run the computation.\n",
        "\n",
        "This allows each building block to keep track of its own **state** (stored at the server) as needed to perform its operations. While it will not be used in this tutorial, it can be used for things like tracking how many iterations have occurred, or keeping track of optimizer states.\n",
        "\n",
        "Client work TF logic should generally be wrapped as a [`tff.learning.templates.ClientWorkProcess`](https://www.tensorflow.org/federated/api_docs/python/tff/learning/templates/ClientWorkProcess), which codifies the expected types going into and out of the client's local training. It can be parameterized by a model and optimizer, as below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "X-I-jPsZmmMy"
      },
      "outputs": [],
      "source": [
        "def build_gradient_clipping_client_work(\n",
        "    model: tff.learning.models.FunctionalModel,\n",
        "    optimizer: tff.learning.optimizers.Optimizer,\n",
        ") -> tff.learning.templates.ClientWorkProcess:\n",
        "  \"\"\"Creates a client work process that uses gradient clipping.\"\"\"\n",
        "  data_type = tff.SequenceType(tff.tensorflow.to_type(model.input_spec))\n",
        "  model_weights_type = tff.types.to_type(\n",
        "      tf.nest.map_structure(\n",
        "          lambda arr: tff.types.TensorType(shape=arr.shape, dtype=arr.dtype),\n",
        "          tff.learning.models.ModelWeights(*model.initial_weights),\n",
        "      )\n",
        "  )\n",
        "\n",
        "  @tff.federated_computation\n",
        "  def initialize_fn():\n",
        "    return tff.federated_value((), tff.SERVER)\n",
        "\n",
        "  @tff.tensorflow.computation(model_weights_type, data_type)\n",
        "  def client_update_computation(model_weights, dataset):\n",
        "    return client_update(model, dataset, model_weights, optimizer)\n",
        "\n",
        "  @tff.federated_computation(\n",
        "      initialize_fn.type_signature.result,\n",
        "      tff.FederatedType(model_weights_type, tff.CLIENTS),\n",
        "      tff.FederatedType(data_type, tff.CLIENTS),\n",
        "  )\n",
        "  def next_fn(state, model_weights, client_dataset):\n",
        "    client_result = tff.federated_map(\n",
        "        client_update_computation, (model_weights, client_dataset)\n",
        "    )\n",
        "    # Return empty measurements, though a more complete algorithm might\n",
        "    # measure something here.\n",
        "    measurements = tff.federated_value((), tff.SERVER)\n",
        "    return tff.templates.MeasuredProcessOutput(\n",
        "        state, client_result, measurements\n",
        "    )\n",
        "\n",
        "  return tff.learning.templates.ClientWorkProcess(initialize_fn, next_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMUX0d0Sx1Gq"
      },
      "source": [
        "# Composing a Learning Algorithm\n",
        "\n",
        "Let's put the client work above into a full-fledged algorithm. First, let's set up our data and model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ_N9XbULo8P"
      },
      "source": [
        "## Preparing the input data\n",
        "Load and preprocess the EMNIST dataset included in TFF. For more details, see the [image classification](federated_learning_for_image_classification.ipynb) tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-WdnFluLLo8P"
      },
      "outputs": [],
      "source": [
        "emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq8893GogB8E"
      },
      "source": [
        "In order to feed the dataset into our model, the data is flattened and converted into tuples of the form `(flattened_image_vector, label)`.\n",
        "\n",
        "Let's select a small number of clients, and apply the preprocessing above to their datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Blrh8zJgLo8R"
      },
      "outputs": [],
      "source": [
        "NUM_CLIENTS = 10\n",
        "BATCH_SIZE = 20\n",
        "\n",
        "def preprocess(dataset):\n",
        "\n",
        "  def batch_format_fn(element):\n",
        "    \"\"\"Flatten a batch of EMNIST data and return a (features, label) tuple.\"\"\"\n",
        "    return (tf.reshape(element['pixels'], [-1, 784]),\n",
        "            tf.reshape(element['label'], [-1, 1]))\n",
        "\n",
        "  return dataset.batch(BATCH_SIZE).map(batch_format_fn)\n",
        "\n",
        "client_ids = sorted(emnist_train.client_ids)[:NUM_CLIENTS]\n",
        "federated_train_data = [preprocess(emnist_train.create_tf_dataset_for_client(x))\n",
        "  for x in client_ids\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNO_Y9j_Lo8X"
      },
      "source": [
        "## Preparing the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJ0I89ixz8yV"
      },
      "source": [
        "This uses the same model as in the [image classification](federated_learning_for_image_classification.ipynb) tutorial. This model (implemented via `tf.keras`) has a single hidden layer, followed by a softmax layer. In order to use this model in TFF, Keras model is wrapped as a [`tff.learning.models.FunctionalModel`](https://www.tensorflow.org/federated/api_docs/python/tff/learning/models/FunctionalModel). This allows us to perform the model's [forward pass](https://www.tensorflow.org/federated/api_docs/python/tff/learning/models/FunctionalModel#predict_on_batch)\n",
        "aggregator_factory = tff.aggregators.MeanFactory()\n",
        "aggregator = aggregator_factory.create(\n",
        "    model_weights_type.trainable, tff.TensorType(np.float32)\n",
        ")\n",
        "finalizer = tff.learning.templates.build_apply_optimizer_finalizer(\n",
        "    server_optimizer, model_weights_type\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rw6_YzhedY1f"
      },
      "outputs": [],
      "source": [
        "initializer = tf.keras.initializers.GlorotNormal(seed=0)\n",
        "keras_model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(shape=(784,)),\n",
        "    tf.keras.layers.Dense(10, kernel_initializer=initializer),\n",
        "    tf.keras.layers.Softmax(),\n",
        "])\n",
        "\n",
        "tff_model = tff.learning.models.functional_model_from_keras(\n",
        "    keras_model,\n",
        "    loss_fn=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    input_spec=federated_train_data[0].element_spec,\n",
        "    metrics_constructor=collections.OrderedDict(\n",
        "        accuracy=tf.keras.metrics.SparseCategoricalAccuracy\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LT4ORiiIddoU"
      },
      "source": [
        "## Preparing the optimizers\n",
        "\n",
        "Just as in [`tff.learning.algorithms.build_weighted_fed_avg`](https://www.tensorflow.org/federated/api_docs/python/tff/learning/algorithms/build_weighted_fed_avg), there are two optimizers here: A client optimizer, and a server optimizer. For simplicity, the optimizers will be SGD with different learning rates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "S6lP8k0Mdf5F"
      },
      "outputs": [],
      "source": [
        "client_optimizer = tff.learning.optimizers.build_sgdm(learning_rate=0.01)\n",
        "server_optimizer = tff.learning.optimizers.build_sgdm(learning_rate=1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO5WvBIudhj_"
      },
      "source": [
        "*italicized text*## Defining the building blocks\n",
        "\n",
        "Now that the client work building block, data, model, and optimizers are set up, it remains to create building blocks for the distributor, the aggregator, and the finalizer. This can be done just by borrowing some defaults available in TFF and that are used by FedAvg."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_federated as tff\n",
        "import tensorflow as tf\n",
        "from tensorflow_federated.python.learning import templates\n",
        "\n",
        "def build_gradient_clipping_client_work(\n",
        "    model: tff.learning.models.FunctionalModel,\n",
        "    optimizer: tff.learning.optimizers.Optimizer,\n",
        ") -> tff.learning.templates.ClientWorkProcess:\n",
        "  \"\"\"Creates a client work process that uses gradient clipping.\"\"\"\n",
        "  # Use tff.types.from_structure to convert the input spec structure to a TFF type\n",
        "  data_type = tff.types.from_structure(model.input_spec)\n",
        "  seq_type = tff.types.SequenceType(data_type)\n",
        "\n",
        "  # Get the model weights type from the model's initial weights (Corrected)\n",
        "  model_weights_type = tff.types.to_type(\n",
        "      tf.nest.map_structure(\n",
        "          lambda arr: tff.types.TensorType(shape=arr.shape, dtype=arr.dtype),\n",
        "          tff.learning.models.ModelWeights(*model.initial_weights),\n",
        "      )\n",
        "  )\n",
        "\n",
        "\n",
        "  @tff.federated_computation\n",
        "  def initialize_fn():\n",
        "    return tff.federated_value((), tff.SERVER)\n",
        "\n",
        "  # Use the corrected data_type in the tensorflow computation\n",
        "  @tff.tensorflow.computation(model_weights_type, seq_type)\n",
        "  def client_update_computation(model_weights, dataset):\n",
        "    return client_update(model, dataset, model_weights, optimizer)\n",
        "\n",
        "  @tff.federated_computation(\n",
        "      initialize_fn.type_signature.result,\n",
        "      tff.FederatedType(model_weights_type, tff.CLIENTS),\n",
        "      tff.FederatedType(seq_type, tff.CLIENTS), # Use the corrected seq_type\n",
        "  )\n",
        "  def next_fn(state, model_weights, client_dataset):\n",
        "    client_result = tff.federated_map(\n",
        "        client_update_computation, (model_weights, client_dataset)\n",
        "    )\n",
        "    # Return empty measurements, though a more complete algorithm might\n",
        "    # measure something here.\n",
        "    measurements = tff.federated_value((), tff.SERVER)\n",
        "    return tff.templates.MeasuredProcessOutput(\n",
        "        state, client_result, measurements\n",
        "    )\n",
        "\n",
        "  return tff.learning.templates.ClientWorkProcess(initialize_fn, next_fn)\n"
      ],
      "metadata": {
        "id": "4poZw41m3lxm"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "\n",
        "# 1. Model creation function\n",
        "def create_keras_model():\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(784,)),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "# 2. Input specification\n",
        "input_spec = (\n",
        "    tf.TensorSpec(shape=[None, 784], dtype=tf.float32),\n",
        "    tf.TensorSpec(shape=[None], dtype=tf.int32)\n",
        ")\n",
        "\n",
        "# 3. Model function for TFF\n",
        "def model_fn():\n",
        "    return tff.learning.models.from_keras_model(\n",
        "        keras_model=create_keras_model(),\n",
        "        input_spec=input_spec,\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        "    )\n",
        "\n",
        "# 4. Optimizers\n",
        "client_optimizer = tff.learning.optimizers.build_sgdm(learning_rate=0.01)\n",
        "server_optimizer = tff.learning.optimizers.build_sgdm(learning_rate=1.0)\n",
        "\n",
        "# 5. Get model weights type\n",
        "@tff.tensorflow.computation\n",
        "def initial_model_weights_fn():\n",
        "  # Instantiate the model to get its initial weights and return as ModelWeights\n",
        "  model = model_fn()\n",
        "  return tff.learning.models.ModelWeights.from_model(model)\n",
        "\n",
        "\n",
        "model_weights_type = initial_model_weights_fn.type_signature.result\n",
        "\n",
        "\n",
        "# 6. Build distributor\n",
        "distributor = tff.learning.templates.build_broadcast_process(model_weights_type)\n",
        "\n",
        "# 7. Build client work using the correct function\n",
        "# Instantiate the model *outside* the function call\n",
        "tff_model_instance = model_fn()\n",
        "client_work = build_gradient_clipping_client_work(tff_model_instance, client_optimizer)\n",
        "\n",
        "\n",
        "# 8. Build aggregator\n",
        "aggregator_factory = tff.aggregators.MeanFactory()\n",
        "aggregator = aggregator_factory.create(\n",
        "    model_weights_type.trainable,\n",
        "    tff.TensorType(tf.float32)\n",
        ")\n",
        "\n",
        "# 9. Build finalizer\n",
        "finalizer = tff.learning.templates.build_apply_optimizer_finalizer(\n",
        "    server_optimizer,\n",
        "    model_weights_type\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "lPMnmYmAnIKM",
        "outputId": "151a8a6c-fffb-4fdf-94e5-de3faca9db8e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'tensorflow_federated.python.core.impl.types' has no attribute 'from_structure'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-20-1301872073.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Instantiate the model *outside* the function call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mtff_model_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mclient_work\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_gradient_clipping_client_work\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtff_model_instance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-19-3687423178.py\u001b[0m in \u001b[0;36mbuild_gradient_clipping_client_work\u001b[0;34m(model, optimizer)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;34m\"\"\"Creates a client work process that uses gradient clipping.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# Use tff.types.from_structure to convert the input spec structure to a TFF type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mdata_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mseq_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequenceType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow_federated.python.core.impl.types' has no attribute 'from_structure'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function that creates a Keras model\n",
        "def create_keras_model():\n",
        "    model = tf.keras.models.Sequential([\n",
        "        # Add your model layers here (example)\n",
        "        tf.keras.layers.Dense(10, activation='softmax', input_shape=(784,))\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Define input specifications for your data\n",
        "input_spec = (\n",
        "    tf.TensorSpec(shape=[None, 784], dtype=tf.float32),  # Features\n",
        "    tf.TensorSpec(shape=[None], dtype=tf.int32)          # Labels\n",
        ")\n",
        "\n",
        "# Create a TFF model wrapper from Keras model\n",
        "tff_model = tff.learning.models.from_keras_model(\n",
        "    keras_model=create_keras_model(),\n",
        "    input_spec=input_spec,\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        ")\n",
        "\n",
        "# Define model_fn for client work (must be a callable without arguments)\n",
        "def model_fn():\n",
        "    return tff.learning.models.from_keras_model(\n",
        "        keras_model=create_keras_model(),\n",
        "        input_spec=input_spec,\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        "    )\n",
        "\n",
        "# Define client optimizer function\n",
        "def client_optimizer_fn():\n",
        "    return tf.keras.optimizers.SGD(learning_rate=0.02)  # Example optimizer\n",
        "\n",
        "# --- Original code below (now fixed) ---\n",
        "@tff.tf_computation  # Changed from tensorflow.computation to tf_computation\n",
        "def initial_model_weights_fn():\n",
        "    return tff.learning.models.ModelWeights(*tff_model.initial_weights)\n",
        "\n",
        "model_weights_type = initial_model_weights_fn.type_signature.result\n",
        "\n",
        "distributor = tff.learning.templates.build_broadcast_process(model_weights_type)\n",
        "client_work = build_gradient_clipping_client_work(model_fn, client_optimizer_fn)\n",
        "\n",
        "aggregator_factory = tff.aggregators.MeanFactory()\n",
        "aggregator = aggregator_factory.create(\n",
        "    model_weights_type.trainable,\n",
        "    tff.TensorType(tf.float32)  # Use tf.float32 instead of np.float32\n",
        ")\n",
        "finalizer = tff.learning.templates.build_apply_optimizer_finalizer(\n",
        "    server_optimizer_fn,  # This must be defined elsewhere\n",
        "    model_weights_type\n",
        ")"
      ],
      "metadata": {
        "id": "Q86npabLIEzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKxDA6-qdjHi"
      },
      "outputs": [],
      "source": [
        "@tff.tensorflow.computation\n",
        "def initial_model_weights_fn():\n",
        "  return tff.learning.models.ModelWeights(*tff_model.initial_weights)\n",
        "\n",
        "\n",
        "model_weights_type = initial_model_weights_fn.type_signature.result\n",
        "\n",
        "distributor = tff.learning.templates.build_broadcast_process(model_weights_type)\n",
        "client_work = build_gradient_clipping_client_work(model_fn, client_optimizer_fn)\n",
        "\n",
        "# TFF aggregators use a factory pattern, which create an aggregator\n",
        "# based on the output type of the client work. This also uses a float (the number\n",
        "# of examples) to govern the weight in the average being computed.)\n",
        "aggregator_factory = tff.aggregators.MeanFactory()\n",
        "aggregator = aggregator_factory.create(\n",
        "    model_weights_type.trainable, tff.TensorType(np.float32)\n",
        ")\n",
        "finalizer = tff.learning.templates.build_apply_optimizer_finalizer(\n",
        "    server_optimizer, model_weights_type\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEYYNHqI1Jif"
      },
      "source": [
        "## Composing the building blocks\n",
        "\n",
        "Finally, you can use a built-in **composer** in TFF for putting the building blocks together. This one is a relatively simple composer, which takes the 4 building blocks above and wires their types together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_86iNeM0IBm"
      },
      "outputs": [],
      "source": [
        "fed_avg_with_clipping = tff.learning.templates.compose_learning_process(\n",
        "    initial_model_weights_fn,\n",
        "    distributor,\n",
        "    client_work,\n",
        "    aggregator,\n",
        "    finalizer\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcK69pCG16-E"
      },
      "source": [
        "# Running the algorithm\n",
        "\n",
        "Now that the algorithm is done, let's run it. First, **initialize** the algorithm. The **state** of this algorithm has a component for each building block, along with one for the *global model weights*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jg22oFx11YKK"
      },
      "outputs": [],
      "source": [
        "state = fed_avg_with_clipping.initialize()\n",
        "\n",
        "state.client_work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmCiEdoq2doJ"
      },
      "source": [
        "As expected, the client work has an empty state (remember the client work code above!). However, other building blocks may have non-empty state. For example, the finalizer keeps track of how many iterations have occurred. Since `next` has not been run yet, it has a state of `0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEuB-8Z71-bd"
      },
      "outputs": [],
      "source": [
        "state.finalizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2N9XObhZ2zSQ"
      },
      "source": [
        "Now run a training round."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKhPuBgW1-3c"
      },
      "outputs": [],
      "source": [
        "learning_process_output = fed_avg_with_clipping.next(state, federated_train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7L0jKEe29bk"
      },
      "source": [
        "The output of this (`tff.learning.templates.LearningProcessOutput`) has both a `.state` and `.metrics` output. Let's look at both."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMsBmmQz28AZ"
      },
      "outputs": [],
      "source": [
        "learning_process_output.state.finalizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwcfhAbP3VkH"
      },
      "source": [
        "Clearly, the finalizer state has incremented by one, as one round of `.next` has been run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0K91G_Ob3E05"
      },
      "outputs": [],
      "source": [
        "learning_process_output.metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sDyO9uz3Jaz"
      },
      "source": [
        "While the metrics are empty, for more complex and practical algorithms they'll generally be full of useful information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPpxe7Ie3gLJ"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8uEZw-T3iBB"
      },
      "source": [
        "By using the building block/composers framework above, you can create entirely new learning algorithms, without having to re-do everything from scratch. However, this is only the starting point. This framework makes it much easier to express algorithms as simple modifications of FedAvg. For more algorithms, see [`tff.learning.algorithms`](https://www.tensorflow.org/federated/api_docs/python/tff/learning/algorithms), which contains algorithms such as [FedProx](https://www.tensorflow.org/federated/api_docs/python/tff/learning/algorithms/build_weighted_fed_prox) and [FedAvg with client learning rate scheduling](https://www.tensorflow.org/federated/api_docs/python/tff/learning/algorithms/build_weighted_fed_avg_with_optimizer_schedule). These APIs can even aid implementations of entirely new algorithms, such as [federated k-means clustering](https://www.tensorflow.org/federated/api_docs/python/tff/learning/algorithms/build_fed_kmeans)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY0ktCY7hi_2"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "composing_learning_algorithms.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}